# 表格型方法

使用查找表的强化学习方法称为表格型方法（tabular method），如蒙特卡洛、Q学习和Sarsa

## 马尔可夫决策过程

马尔可夫决策过程也是强化学习里面一个非常基本的学习框架。状态、动作、状态转移概率和奖励$(S,A,P,R)$,这4个合集就构成了强化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组。

状态转移概率是具有马尔可夫性质的（系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往任何状态）。

### 有模型

环境可以用概率函数$P[s_{t+1},r_t|s_t,a_t]$和奖励函数$R[s_t,a_t]$来描述。概率函数就是状态转移的概率，它反映的是环境的随机性。

- 如果知道概率函数和奖励函数，马尔可夫决策过程就是已知的，我们可以通过策略迭代和价值迭代来找最佳的策略。
- 如果知道环境的状态转移概率和奖励函数，可以认为这个环境是已知的，这时可以用动态规划算法去计算。

### 免模型

很多强化学习的经典算法都是免模型的，也就是环境是未知的，也就是这一系列的决策的概率函数和奖励函数是未知的，这就是有模型与免模型的最大的区别。

强化学习可以应用于完全未知的和随机的环境。强化学习像人类一样学习，人类通过尝试不同的路来学习，通过尝试不同的路，人类可以慢慢地了解哪个状态会更好。强化学习用价值函数$V(S)$来表示状态是好的还是坏的，用 Q 函数来判断在什么状态下采取什么动作能够取得最大奖励，即用 Q 函数来表示状态-动作值。

![免模型试错探索图](https://datawhalechina.github.io/easy-rl/img/ch3/3.3.png)

#### 动态规划方法、蒙特卡洛方法以及时序差分方法的自举和采样

自举是指更新时使用了估计。

采样是指更新时通过采样得到一个期望。

##### 动态规划方法

动态规划方法直接计算期望，它把所有相关的状态都进行加和，即
$$V(s_t)\larr \mathbb{E}_{\pi}[r_{t+1}+\gamma V(s_{t+1})]$$

![动态规划方法](https://datawhalechina.github.io/easy-rl/img/ch3/comparison_2.png)

##### 蒙特卡洛方法
蒙特卡洛方法在当前状态下，采取一条支路，在这条路径上进行更新，更新这条路径上的所有状态，即
$$V(s_t)\larr V(s_t)+\alpha(G_t-V(s_t))$$

![蒙特卡洛方法](https://datawhalechina.github.io/easy-rl/img/ch3/comparison_3.png)

##### 时序差分方法
时序差分从当前状态开始，往前走了一步，关注的是非常局部的步骤，即
$$TD(0):V(s_t)\larr V(s_t)+\alpha(r_{t+1}+\gamma V(s_{t+1})-V(s_t))$$

![时序差分方法](https://datawhalechina.github.io/easy-rl/img/ch3/comparison_4.png)

##### 统一视角

![统一视角](https://datawhalechina.github.io/easy-rl/img/ch3/comparison_5.png)

### 总结

![总结](https://datawhalechina.github.io/easy-rl/img/ch3/3.21.png)

#### 参考文献

1、[蘑菇书EasyRL](https://datawhalechina.github.io/easy-rl/#/)







