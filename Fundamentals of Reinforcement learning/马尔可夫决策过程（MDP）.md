# 马尔可夫决策过程

## 关键词

- **马尔可夫性质（Markov property, MP）**：如果某一个过程未来的状态与过去的状态无关，只由现在的状态决定，那么其具有马尔可夫性质。换句话说，一个状态的下一个状态只取决于它的当前状态，而与它当前状态之前的状态都没有关系。
- **马尔可夫链（Markov chain）**：概率论和数理统计中具有马尔可夫性质且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。
- **状态转移矩阵（state transition matrix）**：状态转移矩阵类似于条件概率（conditional probability），其表示当智能体到达某状态后，到达其他所有状态的概率。矩阵的每一行描述的是从某节点到达所有其他节点的概率。
- **马尔可夫奖励过程（Markov reward process，MRP）**： 本质是马尔可夫链加上一个奖励函数。在马尔可夫奖励过程中，状态转移矩阵和它的状态都与马尔可夫链的一样，只多了一个奖励函数。奖励函数是一个期望，即在某一个状态可以获得多大的奖励。
- **范围（horizon）**：定义了同一个回合（episode）或者一个完整轨迹的长度，它是由有限个步数决定的。
- **回报（return）**：把奖励进行折扣（discounted），然后获得的对应的奖励。
- **贝尔曼方程（Bellman equation）**：其定义了当前状态与未来状态的迭代关系，表示当前状态的价值函数可以通过下个状态的价值函数来计算。贝尔曼方程因其提出者、动态规划创始人理查德 贝尔曼（Richard Bellman）而得名，同时也被叫作“**动态规划方程**”。贝尔曼方程即$V(s)=R(s)+\gamma\sum_{s' \in S}P(s'|s)V(s')$,矩阵形式为$V=R+\gamma PV$。
- **蒙特卡洛算法（Monte Carlo algorithm，MC algorithm）**： 可用来计算价值函数的值。当得到一个马尔可夫奖励过程后，我们可以从某一个状态开始，把物体放到环境中，让它随波流动，这样就会产生一个轨迹，从而得到一个折扣后的奖励$g$。当积累该奖励到一定数量后，用它直接除以轨迹数量，就会得到其价值函数的值。
- **动态规划算法（dynamic programming，DP）**： 其可用来计算价值函数的值。通过一直迭代对应的贝尔曼方程，最后使其收敛。当最后更新的状态与上一个状态差距不大的时候，动态规划算法的更新就可以停止。
- **Q函数（Q-function）**： 其定义的是某一个状态和某一个动作所对应的有可能得到的回报的期望。
- **马尔可夫决策过程中的预测问题**：即策略评估问题，给定一个马尔可夫决策过程以及一个策略$\pi$，计算它的策略函数，即每个状态的价值函数值是多少。其可以通过动态规划算法解决。
- **马尔可夫决策过程中的控制问题**：即寻找一个最佳策略，其输入是马尔可夫决策过程，输出是最佳价值函数（optimal value function）以及最佳策略（optimal policy）。其可以通过动态规划算法解决。
- **最佳价值函数**：搜索一种策略$\pi$，使每个状态的价值最大，$V^*$就是到达每一个状态的极大值。在极大值中，我们得到的策略是最佳策略。最佳策略使得每个状态的价值函数都取得最大值。所以当我们说某一个马尔可夫决策过程的环境可解时，其实就是我们可以得到一个最佳价值函数。

## 一些问答
- 如何寻找最佳策略，寻找最佳策略方法有哪些？
  - 本质来说，当我们取得最佳价值函数后，我们可以通过对Q函数进行最大化，从而得到最佳价值。然后，我们直接对Q函数取一个让动作最大化的值，就可以直接得到其最佳策略。具体方法如下：
  - 穷举法（一般不使用）：假设我们有有限个状态、有限个动作可能性，那么每个状态我们可以采取$A$种动作策略，那么总共就是$|A|^{|S|}$个可能的策略。我们可以把他们穷举一遍，然后算出每种策略的价值函数，对比一下就可以得到最佳策略。但是这种方法的效率极低。
  - **策略迭代**： 一种迭代方法，其由两部分组成，以下两个步骤一直在迭代进行，最终收敛，其过程有些类似于机器学习中的EM算法（期望-最大化算法）。第一个步骤是策略评估，即当前我们在优化这个策略$\pi$，在优化过程中通过评估从而得到一个更新的策略；第二个步骤是策略提升，即取得价值函数后，进一步推算出它的Q函数，得到它的最大值。
  - **价值迭代**： 我们一直迭代贝尔曼最优方程，通过迭代，其能逐渐趋向于最佳策略，这是价值迭代方法的核心。我们为了得到最佳的$V^*$，对于每个状态的$V^*$值，直接使用贝尔曼最优方程进行迭代，迭代多次之后它就会收敛到最佳策略及其对应的状态，这里是没有策略函数的。
- 如果数据流不具备马尔可夫性质怎么办？应该如何处理？
  - 如果不具备马尔可夫性，即下一个状态与之前的状态也有关，若仅用当前的状态来求解决策过程，势必导致决策的泛化能力变差。为了解决这个问题，可以利用循环神经网络对历史信息建模，获得包含历史信息的状态表征，表征过程也可以使用注意力机制等手段，最后在表征状态空间求解马尔可夫决策过程问题。